#!/usr/bin/env python3
"""
Backfill Missing Files From S3 to GCS
------------------------------------
This script reads a JSON report (generated by any of the migration checker
scripts) and uploads the files marked as **missing** to GCS.

Usage
-----
python s3_to_gcs_backfill.py <report_json> <s3_bucket_env> <gcs_bucket_env> [max_workers]

Example:
python s3_to_gcs_backfill.py migration_check_rpc_optimism_optimism_.json S3_LONG_TERM_BUCKET GCS_BUCKET_NAME 8

Environment variables specified will be looked up to get bucket names.
You can reuse the same .env file as the other scripts.
"""
import os
import sys
import json
import time
from datetime import datetime
import concurrent.futures
from tqdm import tqdm
from dotenv import load_dotenv

# Load env vars
load_dotenv()

# Import helper functions from the main transfer script
from s3_to_gcs_transfer import (
    read_parquet_from_s3,
    safe_save_data_for_range,
    extract_block_info,
    check_file_exists_in_gcs,
)


def load_missing_files(report_path):
    """Return list of missing file keys from the checker report."""
    with open(report_path, "r") as f:
        data = json.load(f)

    missing = [r["s3_file"] for r in data.get("results", []) if r.get("status") == "missing"]
    return missing


def process_missing_file(args):
    """Process a single missing file: download from S3, upload to GCS."""
    (
        file_key,
        s3_bucket,
        gcs_bucket,
        chain,
    ) = args

    try:
        # Extract block range
        block_start, block_end = extract_block_info(file_key)
        if block_start is None:
            return False, file_key, "could-not-extract-block-info"

        # Read from S3
        df, _, _ = read_parquet_from_s3(s3_bucket, file_key)
        if df is None or df.empty:
            return False, file_key, "empty-or-read-error"

        # Double-check if exists in GCS
        exists, _ = check_file_exists_in_gcs(df, block_start, block_end, chain, gcs_bucket)
        if exists:
            return True, file_key, "already-exists"

        # Save to GCS
        success = safe_save_data_for_range(df, block_start, block_end, chain, gcs_bucket)
        return success, file_key, "uploaded" if success else "upload-failed"

    except Exception as e:
        return False, file_key, str(e)


def main():
    if len(sys.argv) < 4:
        print(
            "Usage: python s3_to_gcs_backfill.py <report_json> <s3_bucket_env_var> <gcs_bucket_env_var> [max_workers]"
        )
        sys.exit(1)

    report_json = sys.argv[1]
    s3_env = sys.argv[2]
    gcs_env = sys.argv[3]
    max_workers = int(sys.argv[4]) if len(sys.argv) >= 5 else 8

    s3_bucket = os.getenv(s3_env)
    gcs_bucket = os.getenv(gcs_env)

    if not s3_bucket or not gcs_bucket:
        print(f"Environment variables {s3_env} or {gcs_env} not set")
        sys.exit(1)

    # Infer chain from report filename (before first underscore after report type)
    chain = None
    try:
        parts = os.path.basename(report_json).split("_")
        # report prefix like migration_check_rpc_{chain}_...
        chain = parts[3]
    except IndexError:
        chain = input("Chain name could not be inferred. Please type chain name: ")

    missing_files = load_missing_files(report_json)
    if not missing_files:
        print("No missing files found in report. Nothing to backfill.")
        return

    print(
        f"Found {len(missing_files)} missing files in report. Starting backfill with {max_workers} workers..."
    )

    start_time = time.time()
    success_count = 0
    skipped_count = 0
    error_count = 0

    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        args_iter = (
            (file_key, s3_bucket, gcs_bucket, chain) for file_key in missing_files
        )
        futures = [executor.submit(process_missing_file, args) for args in args_iter]

        for f in tqdm(concurrent.futures.as_completed(futures), total=len(futures), unit="file"):
            success, file_key, note = f.result()
            if success:
                if note == "already-exists":
                    skipped_count += 1
                else:
                    success_count += 1
            else:
                error_count += 1
                print(f"Error with {file_key}: {note}")

    elapsed = time.time() - start_time
    print("\n" + "=" * 60)
    print("BACKFILL SUMMARY")
    print(f"Uploaded: {success_count}")
    print(f"Skipped (already existed): {skipped_count}")
    print(f"Errors: {error_count}")
    print(f"Time elapsed: {elapsed:.2f}s")
    print("=" * 60)


if __name__ == "__main__":
    main() 

# python s3_to_gcs_backfill.py migration_check_rpc_optimism_optimism_.json S3_LONG_TERM_BUCKET GCS_BUCKET_NAME 12